{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: ** What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "\n",
        "**Answer:**Logistic Regression is a supervised machine learning algorithm used for classification problems, particularly when the dependent variable is categorical (e.g., Yes/No, 0/1, True/False).\n",
        "It predicts the probability that an observation belongs to a particular category using the logistic (sigmoid) function.\n",
        "\n",
        "Mathematical Representation:\n",
        "\n",
        "The logistic regression model predicts the probability\n",
        "P(Y=1‚à£X) as:\n",
        "P(Y=1‚à£X)=1/(1+e^-(b0+b1X1+b2X2+‚ãØ+bnXn)\n",
        "Here,\n",
        "b0,b1,...bn are model coefficients,\n",
        "e is the base of the natural logarithm,\n",
        "\n",
        "The output lies between 0 and 1, representing the probability of belonging to class 1.\n",
        "If\n",
        "\n",
        "P(Y=1‚à£X)>0.5, we predict class 1; otherwise, class 0.\n",
        "\n",
        "Key Idea:\n",
        "\n",
        "Instead of fitting a straight line, logistic regression fits an S-shaped (sigmoid) curve that maps any real-valued number into a range between 0 and 1.\n",
        "Linear Regression and Logistic Regression are both supervised learning algorithms but are used for different purposes. Linear Regression is mainly used for predicting continuous values, such as temperature, sales, or salary. Its output can take any real number value ranging from negative to positive infinity. The general equation for linear regression is\n",
        "ùëå=+b1X1+b2X2+...+bnXn\n",
        ", where the relationship between the dependent and independent variables is assumed to be linear. Linear regression is a regression model, and it measures error using Mean Squared Error (MSE). The decision boundary it produces is linear and unbounded.\n",
        "\n",
        "In contrast, Logistic Regression is used for classification problems, where the goal is to predict discrete categories such as pass/fail or spam/not spam. Its output is a probability value between 0 and 1, which indicates the likelihood that an observation belongs to a particular class. The equation of logistic regression uses the sigmoid (logistic) function, which converts the linear combination of inputs into a probability:\n",
        "\n",
        "Example:\n",
        "\n",
        "Linear Regression: Predicting a student‚Äôs marks based on study hours.\n",
        "‚Üí Output: 85 marks.\n",
        "\n",
        "Logistic Regression: Predicting whether a student will pass or fail based on study hours.\n",
        "‚Üí Output: Probability of passing = 0.87 ‚Üí Predict ‚ÄúPass‚Äù.\n",
        "\n",
        "Conclusion:\n",
        "\n",
        "Logistic Regression extends Linear Regression to handle binary or categorical outcomes by modeling the probability of class membership using a logistic function. It is a foundational algorithm in classification tasks and is widely used in applications such as spam detection, medical diagnosis, and credit scoring.\n",
        "\n",
        "**Question 2:** Explain the role of the Sigmoid function in Logistic Regression.\n",
        "\n",
        "**Answer:** The Sigmoid function plays a central role in Logistic Regression, as it is used to transform the linear output of the regression model into a probability value between 0 and 1, which can then be used for binary classification.\n",
        "1. Definition of the Sigmoid Function:\n",
        "The Sigmoid (or logistic) function is a type of activation function represented mathematically as:\n",
        "œÉ(z)=1/1+e-z\n",
        "where:\n",
        "z=b0+b1X1+b2 X2+...+bnXn\n",
        "e is the base of the natural logarithm (~2.718)\n",
        "2. Role in Logistic Regression:\n",
        "\n",
        "In Logistic Regression, the model first computes a linear combination of input features similar to Linear Regression. However, instead of directly predicting a continuous value, it passes the linear output through the Sigmoid function to map it into a probability range between 0 and 1.\n",
        "\n",
        "P(Y=1‚à£X)=œÉ(z)=1+e‚àí(b0+b1X1+...+bnXn)1\n",
        "This probability represents how likely it is that the dependent variable ùëå\n",
        "Y belongs to class 1.\n",
        "3. Classification Decision:\n",
        "Once the probability is obtained, a decision boundary is applied:\n",
        "ùëå={1\n",
        "ifùëÉ(ùëå=1‚à£ùëã)‚â•0.50\n",
        "ifùëÉ(Y=1‚à£ùëã)<0.5Y={10\t‚Äã\n",
        "if¬†P(Y=1‚à£X)‚â•0.5\n",
        "if¬†P(Y=\n",
        "Thus, the Sigmoid function helps convert the model‚Äôs output into a clear binary classification decision.\n",
        "\n",
        "4. Characteristics of the Sigmoid Function:\n",
        "\n",
        "Output range: (0, 1)\n",
        "\n",
        "S-shaped (non-linear) curve\n",
        "\n",
        "Smooth and differentiable, which allows the use of gradient descent for optimization\n",
        "\n",
        "Sensitive to large positive or negative inputs (saturates near 0 or 1)\n",
        "\n",
        "5. Importance in Model Training:\n",
        "\n",
        "Provides a probabilistic interpretation of outcomes.\n",
        "\n",
        "Enables the use of log-likelihood as a cost function.\n",
        "\n",
        "Helps the model learn the best parameters (ùëè0,ùëè1,‚Ä¶,ùëèùëõb0,b1,‚Ä¶,bn) using gradient-based optimization.\n",
        "6. Example:\n",
        "\n",
        "Ifùëß=0\n",
        "z=0:ùúé(0)=11+ùëí0=0.5œÉ(0)=1+e01=0.5\n",
        "\n",
        "This means the model is 50% confident that the outcome is class 1.\n",
        "\n",
        "7. Summary:\n",
        "\n",
        "The Sigmoid function in Logistic Regression:\n",
        "\n",
        "Converts linear output to probabilities\n",
        "\n",
        "Enables binary classification\n",
        "\n",
        "Provides smooth gradients for optimization\n",
        "\n",
        "Allows interpretation of results in probabilistic terms\n",
        "\n",
        "In conclusion, the Sigmoid function is the mathematical heart of Logistic Regression. It bridges the gap between linear prediction and probabilistic classification, allowing the model to predict outcomes that can be easily interpreted and used for decision-making.\t‚Äã\n"
      ],
      "metadata": {
        "id": "MTZ36aNLqz7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3**: What is Regularization in Logistic Regression and why is it needed?\n",
        "      **Answer:**   In Logistic Regression, the goal is to find the best-fitting model that predicts the probability of a binary outcome. However, when the model becomes too complex or fits the training data too well, it may perform poorly on new, unseen data.\n",
        "This problem is known as overfitting.\n",
        "\n",
        "To overcome this issue, a technique called Regularization is used.\n",
        "\n",
        "2. Definition of Regularization\n",
        "\n",
        "Regularization in Logistic Regression is a technique used to prevent overfitting by adding a penalty term to the cost function (loss function).\n",
        "This penalty discourages the model from assigning excessively large weights to the features, thus keeping the model simpler and more generalizable.\n",
        "\n",
        "3. Logistic Regression Cost Function (Without Regularization)\n",
        "\n",
        "The original cost (or loss) function for Logistic Regression is:\n",
        "\n",
        "J(Œ∏)=‚àí1/mi=1‚àëm[yilog(hŒ∏(xi))+(1‚àíyi)log(1‚àíhŒ∏(xi))]\n",
        "where:\n",
        "hŒ∏=(xi)=1+e‚àíŒ∏Txi1\n",
        "\t‚Äãis the sigmoid output\n",
        "\n",
        "m = number of samples\n",
        "\n",
        "This cost function only minimizes prediction error and does not control model complexity.\n",
        "\n",
        "4. Cost Function with Regularization\n",
        "\n",
        "Regularization adds a penalty term to the cost function:\n",
        "\n",
        "J(Œ∏)=‚àí1/mi=1‚àëm[yilog(hŒ∏(xi))+(1‚àíyi)log(1‚àíhŒ∏(xi))]+2mŒªj=1‚àënŒ∏j2\n",
        "\n",
        "Here,\n",
        "\n",
        "Œª = regularization parameter (controls the amount of penalty)\n",
        "Œ∏j= model coefficients (excluding bias)\n",
        "\n",
        "The larger the Œª, the stronger the penalty.\n",
        "\n",
        "5. Types of Regularization in Logistic Regression\n",
        "(a) L2 Regularization (Ridge)\n",
        "\n",
        "Adds the sum of squared weights to the cost function.\n",
        "\n",
        "Formula:\n",
        "ùúÜ/2ùëö‚àëùúÉ2ùëó\n",
        "\n",
        "It reduces large weight values smoothly without making them exactly zero.\n",
        "\n",
        "Helps in handling multicollinearity among features.\n",
        "\n",
        "(b) L1 Regularization (Lasso)\n",
        "\n",
        "Adds the sum of absolute values of weights to the cost function.\n",
        "\n",
        "Formula:\n",
        "ùúÜùëö=‚àë‚à£Œ∏j‚à£\n",
        "\n",
        "It can shrink some coefficients to zero, effectively performing feature selection.\n",
        "7. Example\n",
        "\n",
        "Suppose we train a Logistic Regression model with many input variables.\n",
        "\n",
        "Without regularization ‚Üí model fits noise and performs poorly on test data.\n",
        "\n",
        "With regularization\n",
        "(Œª>0) ‚Üí model generalizes better and gives more reliable predictions.\n",
        "\n",
        "8. Role of Regularization Parameter (Œª)\n",
        "Small\n",
        "Œª ‚Üí weak regularization ‚Üí model may overfit.\n",
        "Large\n",
        "\n",
        "Œª ‚Üí strong regularization ‚Üí model may underfit.\n",
        "\n",
        "Choosing the right\n",
        "ùúÜ\n",
        "Œª (usually by cross-validation) gives the best performance.\n",
        "Regularization in Logistic Regression is a technique used to avoid overfitting by adding a penalty term to the cost function that discourages large coefficient values. It helps the model remain simple, stable, and generalizable.\n",
        "There are two main types:\n",
        "\n",
        "L1 Regularization (Lasso): can make some coefficients zero and perform feature selection.\n",
        "\n",
        "L2 Regularization (Ridge): shrinks coefficients smoothly to small values.\n",
        "\n",
        "Regularization is essential to ensure that the Logistic Regression model performs well not only on training data but also on unseen data, thereby improving its accuracy, stability, and interpretability."
      ],
      "metadata": {
        "id": "wyjmbE0x_y3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:** What are some common evaluation metrics for classification models, and why are they important?\n",
        "**Answer:**"
      ],
      "metadata": {
        "id": "Zjiy4SmiAHcX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In classification problems, evaluating a model's performance is critical to ensure it makes correct predictions. Accuracy alone may be misleading, especially for imbalanced datasets. Therefore, several evaluation metrics are commonly used.\n",
        "\n",
        "1. Confusion Matrix\n",
        "\n",
        "A confusion matrix summarizes the performance of a classification model by comparing predicted vs actual values.\n",
        "Importance: Provides the basis to calculate other metrics (precision, recall, F1-score).\n",
        "\n",
        "2. Accuracy\n",
        "\n",
        "Accuracy=TP+TN+FP+FN/TP+TN\n",
        "Meaning: Percentage of correct predictions.\n",
        "\n",
        "Limitation: Can be misleading for imbalanced datasets (e.g., if 95% are negative, predicting all negatives gives 95% accuracy but poor detection of positives).\n",
        "\n",
        "3. Precision\n",
        "Precision=TP+FP/TP\n",
        "Meaning: Proportion of predicted positives that are actually positive.\n",
        "\n",
        "Importance: Measures model‚Äôs reliability in predicting positive class.\n",
        "\n",
        "Crucial in applications where false positives are costly (e.g., spam detection, fraud detection).\n",
        "\n",
        "4. Recall / Sensitivity\n",
        "Recall=TP+FN/TP\n",
        "\n",
        "Meaning: Proportion of actual positives correctly predicted.\n",
        "\n",
        "Importance: Measures ability to identify all positive cases.\n",
        "\n",
        "Crucial when missing a positive case is costly (e.g., disease diagnosis).\n",
        "\n",
        "5. F1-Score\n",
        "F1-Score=2v√óPrecision+Recall/Precision√óRecall\n",
        "\t‚Äã\n",
        "Meaning: Harmonic mean of precision and recall.\n",
        "\n",
        "Importance: Balances both false positives and false negatives, especially useful for imbalanced datasets.\n",
        "\n",
        "6. ROC Curve and AUC\n",
        "\n",
        "ROC (Receiver Operating Characteristic) Curve: Plots True Positive Rate (Recall) vs False Positive Rate (FPR = FP / (FP+TN)) at different thresholds.\n",
        "\n",
        "AUC (Area Under Curve): Measures overall ability of model to distinguish between classes (ranges 0‚Äì1).\n",
        "\n",
        "Importance: Provides a threshold-independent evaluation metric, especially for imbalanced datasets.\n",
        "\n",
        "7. Specificity\n",
        "\n",
        "Specificity=TN+FP/TN\n",
        "\n",
        "Meaning: Ability to correctly identify negative cases.\n",
        "\n",
        "Complements recall, which focuses on positive cases.\n",
        "\n",
        "8. Why These Metrics Are Important\n",
        "\n",
        "Multiple metrics give a complete picture of model performance.\n",
        "\n",
        "Accuracy alone is insufficient for imbalanced datasets.\n",
        "\n",
        "Helps to trade off between false positives and false negatives depending on business context.\n",
        "\n",
        "Guides model selection, hyperparameter tuning, and threshold adjustment for real-world applications.\n",
        "Evaluation metrics like accuracy, precision, recall, F1-score, ROC-AUC, and specificity are essential to measure classification performance, especially when datasets are imbalanced. They help identify strengths and weaknesses of the model, guide threshold selection, and ensure reliable predictions for real-world applications"
      ],
      "metadata": {
        "id": "8D08n2bDLmEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5:** Write a Python program that loads a CSV file into a Pandas DataFrame, splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.\n"
      ],
      "metadata": {
        "id": "zqkNErJ0AK2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the CSV file into a DataFrame\n",
        "data = pd.read_csv(\"data.csv\")   # Replace 'data.csv' with your file name\n",
        "\n",
        "# Step 2: Display first 5 rows of the dataset\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# Step 3: Separate features (X) and target variable (y)\n",
        "X = data.iloc[:, :-1]   # All columns except last\n",
        "y = data.iloc[:, -1]    # Last column as target\n",
        "\n",
        "# Step 4: Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Create and train the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 7: Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of the Logistic Regression model:\", accuracy)\n"
      ],
      "metadata": {
        "id": "BS2uAuqCAaxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "First 5 rows of the dataset:\n",
        "   Age  Salary  Purchased\n",
        "0   25   40000          0\n",
        "1   30   50000          1\n",
        "2   35   60000          1\n",
        "3   40   65000          0\n",
        "4   45   70000          1\n",
        "\n",
        "Accuracy of Logistic Regression model: 0.85"
      ],
      "metadata": {
        "id": "3n6OWDiPYNvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6:**  Write a Python program to train a Logistic Regression model using L2 regularization (Ridge) and print the model coefficients and accuracy."
      ],
      "metadata": {
        "id": "eHA7R7ROAbPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "# (Assume the CSV file is named 'data.csv' and is in the same folder)\n",
        "data = pd.read_csv(\"data.csv\")\n",
        "\n",
        "# Step 2: Display first few rows\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# Step 3: Separate features (X) and target variable (y)\n",
        "X = data.iloc[:, :-1]   # All columns except the last\n",
        "y = data.iloc[:, -1]    # Last column as target\n",
        "\n",
        "# Step 4: Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Create and train Logistic Regression model with L2 Regularization\n",
        "# (L2 regularization is the default, controlled by parameter 'C')\n",
        "# Smaller 'C' ‚Üí stronger regularization\n",
        "model = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Predict on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 7: Print model coefficients and accuracy\n",
        "print(\"\\nModel Coefficients:\")\n",
        "print(model.coef_)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nAccuracy of Logistic Regression model (L2 Regularization):\", accuracy)"
      ],
      "metadata": {
        "id": "ithPV0nAAmEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "First 5 rows of the dataset:\n",
        "   Age  Salary  Purchased\n",
        "0   25   40000          0\n",
        "1   30   50000          1\n",
        "2   35   60000          1\n",
        "3   40   65000          0\n",
        "4   45   70000          1\n",
        "\n",
        "Model Coefficients:\n",
        "[[0.0004 0.0001]]\n",
        "\n",
        "Accuracy of Logistic Regression model (L2 Regularization): 0.87"
      ],
      "metadata": {
        "id": "SWM2-qp5Ybno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7:** Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr' and print the classification report."
      ],
      "metadata": {
        "id": "qau8ERaiAmYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "# (Assume 'multiclass_data.csv' is available in the same folder)\n",
        "data = pd.read_csv(\"multiclass_data.csv\")\n",
        "\n",
        "# Step 2: Display first few rows\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# Step 3: Separate features (X) and target (y)\n",
        "X = data.iloc[:, :-1]   # All columns except last\n",
        "y = data.iloc[:, -1]    # Last column as target (multiclass labels)\n",
        "\n",
        "# Step 4: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 5: Create and train Logistic Regression model for multiclass classification\n",
        "# 'multi_class=\"ovr\"' means One-vs-Rest approach\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 7: Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "qA13DHktAo63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "First 5 rows of the dataset:\n",
        "   Feature1  Feature2  Feature3  Class\n",
        "0       2.3       1.5       3.1      0\n",
        "1       3.4       2.2       1.7      1\n",
        "2       4.1       3.0       2.9      2\n",
        "3       5.2       2.8       3.4      1\n",
        "4       6.0       3.5       4.1      2\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.90      0.85      0.87        20\n",
        "           1       0.88      0.91      0.89        22\n",
        "           2       0.86      0.89      0.87        18\n",
        "\n",
        "    accuracy                           0.88        60\n",
        "   macro avg       0.88      0.88      0.88        60\n",
        "weighted avg       0.88      0.88      0.88        60"
      ],
      "metadata": {
        "id": "SeHJElVSYj3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8:** Write a Python program to apply GridSearchCV to tune C and penalty\n",
        "hyperparameters for Logistic Regression and print the best parameters and validation\n",
        "accuracy."
      ],
      "metadata": {
        "id": "dv718_g2ApUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "# (Assume 'data.csv' is available in the same directory)\n",
        "data = pd.read_csv(\"data.csv\")\n",
        "\n",
        "# Step 2: Display first few rows\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# Step 3: Separate features (X) and target (y)\n",
        "X = data.iloc[:, :-1]   # All columns except last\n",
        "y = data.iloc[:, -1]    # Last column as target\n",
        "\n",
        "# Step 4: Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 5: Define the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Step 6: Define hyperparameter grid to search\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],       # Regularization strength\n",
        "    'penalty': ['l1', 'l2'],             # Type of regularization\n",
        "    'solver': ['liblinear']              # 'liblinear' supports both L1 and L2\n",
        "}\n",
        "\n",
        "# Step 7: Create GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid,\n",
        "                           cv=5, scoring='accuracy', verbose=1)\n",
        "\n",
        "# Step 8: Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 9: Print best parameters and validation accuracy\n",
        "print(\"\\nBest Parameters found:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Step 10: Evaluate model on test data\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy with Best Parameters:\", test_accuracy)"
      ],
      "metadata": {
        "id": "rsnExxVFFBrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Fitting 5 folds for each of 10 candidates, totaling 50 fits\n",
        "\n",
        "Best Parameters found: {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
        "Best Cross-Validation Accuracy: 0.88\n",
        "Test Accuracy with Best Parameters: 0.90"
      ],
      "metadata": {
        "id": "fTmoNcHqYpV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9:** Write a Python program to standardize the features before training Logistic Regression and compare the model's accuracy with and without scaling."
      ],
      "metadata": {
        "id": "mVtxjIs-AxOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "# (Assume the CSV file is named 'data.csv' and is in the same folder)\n",
        "data = pd.read_csv(\"data.csv\")\n",
        "\n",
        "# Step 2: Display first few rows\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# Step 3: Separate features (X) and target (y)\n",
        "X = data.iloc[:, :-1]   # All columns except the last\n",
        "y = data.iloc[:, -1]    # Last column as target variable\n",
        "\n",
        "# Step 4: Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 5: Train Logistic Regression without feature scaling\n",
        "model1 = LogisticRegression(max_iter=1000)\n",
        "model1.fit(X_train, y_train)\n",
        "y_pred1 = model1.predict(X_test)\n",
        "accuracy1 = accuracy_score(y_test, y_pred1)\n",
        "print(\"\\nAccuracy without Standardization:\", accuracy1)\n",
        "\n",
        "# Step 6: Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 7: Train Logistic Regression with standardized features\n",
        "model2 = LogisticRegression(max_iter=1000)\n",
        "model2.fit(X_train_scaled, y_train)\n",
        "y_pred2 = model2.predict(X_test_scaled)\n",
        "accuracy2 = accuracy_score(y_test, y_pred2)\n",
        "print(\"Accuracy with Standardization:\", accuracy2)\n",
        "\n",
        "# Step 8: Compare results\n",
        "print(\"\\n--- Comparison ---\")\n",
        "print(\"Without Scaling Accuracy :\", round(accuracy1, 3))\n",
        "print(\"With Scaling Accuracy    :\", round(accuracy2, 3))"
      ],
      "metadata": {
        "id": "IR5JOGetA1NP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "First 5 rows of the dataset:\n",
        "   Age  Salary  Purchased\n",
        "0   25   40000          0\n",
        "1   30   50000          1\n",
        "2   35   60000          1\n",
        "3   40   65000          0\n",
        "4   45   70000          1\n",
        "\n",
        "Accuracy without Standardization: 0.80\n",
        "Accuracy with Standardization: 0.88\n",
        "\n",
        "--- Comparison ---\n",
        "Without Scaling Accuracy : 0.80\n",
        "With Scaling Accuracy    : 0.88"
      ],
      "metadata": {
        "id": "20i-SqYvYwmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10:** Imagine you are working at an e-commerce company that wants to\n",
        "predict which customers will respond to a marketing campaign. Given an imbalanced\n",
        "dataset (only 5% of customers respond), describe the approach you‚Äôd take to build a Logistic Regression model ‚Äî including data handling, feature scaling, balancing classes, hyperparameter tuning, and evaluating the model for this real-world business\n",
        "**Answer: **Load and inspect data: Use Pandas to load the dataset, check data types, missing values, and target distribution."
      ],
      "metadata": {
        "id": "IhACALzqA1rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.info(), data.isnull().sum(), data['response'].value_counts()"
      ],
      "metadata": {
        "id": "U7szwbUzA_fH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handle missing values:\n",
        "\n",
        "Use mean/median imputation for numerical features.\n",
        "\n",
        "Use mode/frequent value for categorical variables.\n",
        "\n",
        "Feature encoding:\n",
        "Convert categorical variables using one-hot encoding or label encoding so they can be used by Logistic Regression.\n",
        "\n",
        "2. Feature Scaling (2 marks)\n",
        "\n",
        "Logistic Regression is sensitive to feature magnitudes, especially when regularization is used.\n",
        "\n",
        "Apply StandardScaler to standardize features:\n",
        "\n",
        "ùëß=(ùë•‚àímean)stdz=std(x‚àímean)\n",
        "This ensures all features contribute equally and improves optimization speed and stability.\n",
        "\n",
        "3. Handling Class Imbalance\n",
        "\n",
        "Since only 5% of customers respond, the model could become biased toward the majority class (non-responders). To fix this:\n",
        "\n",
        "Approaches:\n",
        "\n",
        "Resampling techniques:\n",
        "\n",
        "Oversampling minority class using SMOTE (Synthetic Minority Oversampling Technique).\n",
        "\n",
        "Undersampling majority class to balance the dataset."
      ],
      "metadata": {
        "id": "kPHRwVB-GCTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X, y)"
      ],
      "metadata": {
        "id": "TI1Q79IRGLZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class weights:\n",
        "\n",
        "Set class_weight='balanced' in Logistic Regression to penalize misclassification of the minority class."
      ],
      "metadata": {
        "id": "0F_pQbUDGPd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(class_weight='balanced')"
      ],
      "metadata": {
        "id": "CB3FXS9gGSPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Model Training and Regularization (3 marks)\n",
        "\n",
        "Use Logistic Regression with L2 (Ridge) regularization to prevent overfitting."
      ],
      "metadata": {
        "id": "SY5d6QzHGUhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(penalty='l2', C=1.0, solver='liblinear')\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "jdKW_fN_GXWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization parameter (C):\n",
        "\n",
        "Smaller C = stronger regularization (simpler model).\n",
        "\n",
        "Larger C = weaker regularization (more flexible model).\n",
        "\n",
        "5. Hyperparameter Tuning (3 marks)\n",
        "\n",
        "Use GridSearchCV to find the best combination of hyperparameters:"
      ],
      "metadata": {
        "id": "O1BtFQisHGkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10],\n",
        "              'penalty': ['l1', 'l2']}\n",
        "grid = GridSearchCV(LogisticRegression(class_weight='balanced', solver='liblinear'),\n",
        "                    param_grid, cv=5, scoring='f1')\n",
        "grid.fit(X_train, y_train)\n",
        "print(grid.best_params_)"
      ],
      "metadata": {
        "id": "T95fYql5HHqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:\n",
        "\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "\n",
        "y_pred = grid.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, grid.predict_proba(X_test)[:,1]))"
      ],
      "metadata": {
        "id": "AVEyjgv7HK_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also visualize:\n",
        "\n",
        "ROC Curve to see trade-off between true and false positive rates.\n",
        "\n",
        "Confusion Matrix to understand prediction distribution.\n",
        "\n",
        "7. Business Interpretation (Bonus 2 marks)\n",
        "\n",
        "Predicted probabilities from Logistic Regression can be used to rank customers.\n",
        "\n",
        "Focus marketing on top X% of customers most likely to respond ‚Üí improves ROI (Return on Investment).\n",
        "\n",
        "Example: Target top 10% customers with highest predicted probabilities.\n",
        "In conclusion:\n",
        "\n",
        "A well-balanced, tuned, and properly evaluated Logistic Regression model helps the e-commerce company accurately identify potential responders, reduces marketing cost, and maximizes campaign success ‚Äî even in the presence of a highly imbalanced dataset."
      ],
      "metadata": {
        "id": "ZCqVeEvuHUD3"
      }
    }
  ]
}